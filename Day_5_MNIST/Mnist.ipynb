{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Layers**\n",
    "==\n",
    "\n",
    "So to do image recognition we need to create a convolutional network!\n",
    "\n",
    "This takes photo data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_channels = 1\n",
    "image_height = 28\n",
    "image_width  = 28\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, image_height, image_width, channels, num_classes):\n",
    "        #this input layer holds the image data as is. in the tensor placeholder \n",
    "        self.input_layer = tf.placeholder(dtype=tf.float32, shape=[None, image_height, image_width, channels], name=\"inputs\")\n",
    "        print(self.input_layer.shape)\n",
    "        \n",
    "        #this convolutional layer takes the input data and grabs the kernel_size and breaks\n",
    "        #that size off and passes it through an activation function that will test similarities to the rest of the image,\n",
    "        conv_layer_1 = tf.layers.conv2d(self.input_layer, filters=32, kernel_size=[2, 2], padding=\"same\", activation=tf.nn.relu)\n",
    "        print(conv_layer_1.shape)\n",
    "         \n",
    "        #the pooling layer removes the precision of the image data\n",
    "        pooling_layer_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=[2,2], strides=2)\n",
    "        print(pooling_layer_1.shape)\n",
    "         \n",
    "        #runs a second convolutional layer to take chunk up a second time\n",
    "        conv_layer_2 = tf.layers.conv2d(pooling_layer_1, filters=64, kernel_size=[2, 2], padding=\"same\", activation=tf.nn.relu)\n",
    "        print(conv_layer_2.shape)\n",
    "        \n",
    "        #the pooling layer does the same  shrinkning our# data sets\n",
    "        pooling_layer_2 = tf.layers.max_pooling2d(conv_layer_2, pool_size=[2, 2], strides=2)\n",
    "        print(pooling_layer_2.shape)\n",
    "        \n",
    "        #flatten the data\n",
    "        flattened_pooling = tf.layers.flatten(pooling_layer_2)\n",
    "        print(\"Flattened: \" + str(flattened_pooling.shape) )\n",
    "        \n",
    "        #develops a neural network containing 1024\n",
    "        dense_layer = tf.layers.dense(flattened_pooling, 1024, activation=tf.nn.relu)\n",
    "        print(\"Dense: \" + str(dense_layer.shape))\n",
    "        dropout = tf.layers.dropout(dense_layer, rate=0.4, training=True)\n",
    "        print(\"Dropout: \" + str(dropout.shape))\n",
    "        outputs = tf.layers.dense(dropout, num_classes)\n",
    "        print(\"Outputs: \" + str(outputs.shape))\n",
    "         \n",
    "        self.choice = tf.argmax(outputs, axis=1)\n",
    "        self.probability = tf.nn.softmax(outputs)\n",
    "         \n",
    "        self.labels = tf.placeholder(dtype=tf.float32, name=\"labels\")\n",
    "        self.accuracy, self.accuracy_op = tf.metrics.accuracy(self.labels, self.choice)\n",
    "         \n",
    "        one_hot_labels = tf.one_hot(indices=tf.cast(self.labels, dtype=tf.int32), depth=num_classes)     \n",
    "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits=outputs)\n",
    "         \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2)\n",
    "        self.train_operation = optimizer.minimize(loss=self.loss, global_step=tf.train.get_global_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "(?, 28, 28, 1)\n",
      "(?, 28, 28, 32)\n",
      "(?, 14, 14, 32)\n",
      "(?, 14, 14, 64)\n",
      "(?, 7, 7, 64)\n",
      "Flattened: (?, 3136)\n",
      "Dense: (?, 1024)\n",
      "Dropout: (?, 1024)\n",
      "Outputs: (?, 16)\n",
      "WARNING:tensorflow:From /home/fatcatmat/tenv/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "training_steps = 2000\n",
    "batch_size = 64\n",
    "\n",
    "image_height = 28\n",
    "image_width  = 28\n",
    "\n",
    "color_channels = 1\n",
    "\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "#extracts mnist data\n",
    "\n",
    "train_data = mnist.train.images\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32) \n",
    "#preps the labels for training\n",
    "\n",
    "cnn = CNN(image_height, image_width, color_channels, 16)\n",
    "\n",
    "#prep the evaluation data\n",
    "\n",
    "eval_data = mnist.train.images\n",
    "eval_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "\n",
    "train_data = train_data.reshape(-1, image_height, image_width, color_channels)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "dataset = dataset.shuffle(buffer_size = train_labels.shape[0])\n",
    "dataset =dataset.batch(batch_size)\n",
    "dataset = dataset.repeat()\n",
    "dataset_iterator = dataset.make_initializable_iterator()\n",
    "next_element = dataset_iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(dataset_iterator.initializer)\n",
    "    for step in range(training_steps):\n",
    "        current_batch = sess.run(next_element)\n",
    "        \n",
    "        batch_inputs = current_batch[0]\n",
    "        batch_labels = current_batch[1]\n",
    "        sess.run((cnn.train_operation, cnn.accuracy_op), feed_dict={cnn.input_layer:batch_inputs, cnn.labels:batch_labels})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
